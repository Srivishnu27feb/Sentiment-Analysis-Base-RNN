{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis - Basic RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GFLDAJfJ2EaNt3nIu0TdYaaCkCw_Iuoc",
      "authorship_tag": "ABX9TyMAqAv2RSlBH7/yQikoWNko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srivishnu27feb/Sentiment-Analysis-Base-RNN-Pytorch-/blob/master/Sentiment_Analysis_Basic_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7drXLYudtQnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = \"spacy\") ###Tokenize using spacy (split to words)\n",
        "LABEL = data.LabelField(dtype = torch.float) ###Futher down the line the Loss (criterion) expects 32 bit float but TorchText sets tensors to longTensors(64 bit int) so conversion is required"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDDPu7jguMrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2f8fad89-dd60-4ba7-c185-62d729562f2c"
      },
      "source": [
        "from torchtext import datasets\n",
        "###import IMBD dataset with TEXT as tokens and LABEL as the output tag (pos/neg)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWH098rfug9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a7a9fa14-0955-4ce3-e803-9cf564dbd78c"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Bela', 'made', '9', 'pics', 'for', 'Monogram', ',', 'but', 'it', 'was', 'only', 'at', 'THIS', 'one', ',', 'the', '4TH', ',', 'that', 'things', 'started', 'to', 'come', 'together', '.', 'All', 'the', 'rest', 'in', 'the', 'series', 'would', 'use', 'this', 'one', 'as', 'the', 'essential', 'template', 'for', 'production', ',', 'writing', 'and', 'character', 'development', '.', 'From', 'here', 'on', ',', 'better', 'or', 'worse', ',', 'the', 'series', 'would', 'also', 'deal', 'with', 'one', 'essential', 'theme', ':', 'a', 'scientist', '(', 'usually', 'Bela', ')', 'makes', 'experiments', 'in', 'the', 'basement', 'or', 'the', 'old', 'house', '(', 'sometimes', 'IN', 'the', 'basement', 'in', 'the', 'old', 'house', ')', 'that', 'causes', 'things', 'to', 'go', 'blooey', '.', 'This', 'was', 'also', 'the', 'first', 'time', 'that', 'Art', 'Director', 'Dave', 'Milton', 'got', 'a', 'chance', 'to', 'spread', 'his', 'wings', '.', 'He', 'came', 'on', 'board', 'for', 'BLACK', 'DRAGONS', ',', 'the', 'flick', 'before', ',', 'but', 'THIS', 'one', 'is', 'where', 'he', 'gets', 'to', 'make', 'his', 'craft', 'start', 'to', 'click', '.', 'Lewis', 'made', 'great', 'atmosphere', 'for', 'next', 'to', 'nothing', ',', 'and', 'was', 'around', 'for', 'all', 'the', 'rest', 'of', 'the', 'Monograms', '.', 'Casting', 'is', 'key', 'in', 'these', ',', 'and', 'it', \"'s\", 'a', 'pretty', 'good', 'one', 'B', 'movie', 'wise', ',', 'here', '.', 'You', 'get', 'Barclay', 'and', 'Harlen', '(', 'also', 'from', 'BLACK', 'DRAGONS),along', 'with', 'Russell', ',', 'who', 'would', 'star', 'in', 'Lewtons', \"'\", 'CAT', 'PEOPLE', 'movies', '..', 'and', 'Rosetto', ',', 'from', 'SPOOKS', 'RUN', 'WILD', '...', 'a', 'nice', 'slice', 'of', 'Poverty', 'Row', 'talent', '.', 'If', 'you', 'have', 'limited', 'time', 'and', 'budget', ',', 'start', 'with', 'this', 'one', '...', 'it', 'sums', 'up', 'everything', 'they', 'had', 'learned', 'up', 'to', 'this', 'point', ',', 'and', 'gives', 'you', 'something', 'to', 'compare', 'the', 'rest', 'to', '.', 'The', 'plot', '?', 'Bela', 'steals', 'gland', 'juice', 'to', 'keep', 'his', 'nasty', 'wife', 'young', '.', 'They', 'both', 'like', 'to', 'sleep', 'in', 'coffins', '.', 'If', 'you', 'can', 'read', 'that', 'and', 'smile', ',', 'the', 'rest', 'will', 'be', 'easy', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK9ytK_1v7iV",
        "colab_type": "text"
      },
      "source": [
        "By Default split function split 70:30 ratio so use split function to create train & Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zRZkxFtuktN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA-CVG86wGg_",
        "colab_type": "text"
      },
      "source": [
        "As the unique no of words is 100000 which is one hot encoded and is of 100000 Dimensions so the processing will be slow. So we restrict the vocab to top 25000 words and remaining would be tagged as <unk> (unknown). This vocab list implies only for Training set as the test set would be unknown in the real case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Jd9kO3v3nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD3k9kumxcjD",
        "colab_type": "text"
      },
      "source": [
        "25000 vocab size +  'unk' + 'pad'; pad is to make the size of the sentence uniform across all examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH6GTGu4wkmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a463bc7-d0ca-4df0-a450-b3217c071a10"
      },
      "source": [
        "print(len(TEXT.vocab),len(LABEL.vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25002 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfoW_rwSxNrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f202554a-4d53-46df-9af5-32f769641f9c"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 203136), (',', 193205)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui7Ap9Mkxewu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ede7c697-83bd-4475-b972-46c7fd8447f4"
      },
      "source": [
        "print(TEXT.vocab.itos[:3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaFA1QTUxwKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "374d5cdd-25d0-4d0b-aca0-bb7def16e2cf"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f8f2a1741e0>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKLJr3wWykw7",
        "colab_type": "text"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX84wpqXxy16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xP0WtXFzE57",
        "colab_type": "text"
      },
      "source": [
        "# **Model Building**\n",
        "The next stage is building the model:\n",
        "\n",
        "**The process is as follows:**\n",
        "\n",
        "Convert One hot encoding (sparse Matrix) --> Dense Martix (Embedding) --> RNN/ LSTM/ GRU as input.\n",
        "\n",
        "\n",
        "**Super** keyword in RNN class will allows us to avoid using the base class name explicitly and also in working with Multiple Inheritance\n",
        "\n",
        "**Constructors**/**init** : All the 3 layers (embedding layer, our RNN, and a linear layer) are defined here. This will be invoked first when the object is created for the RNN class. All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "**Embedding Layer:**\n",
        "The embedding layer is used to transform our sparse one-hot vector  into a dense embedding vector. This embedding layer is simply a single fully connected layer. It reduces the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space.\n",
        "\n",
        "**The RNN layer**: It takes in our dense vector and the previous hidden state  ht−1 , which it uses to calculate the next hidden state,  ht .\n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer,  f(hT) , transforming it to the correct output dimension.\n",
        "\n",
        "Forward Method is called when the input examples are fed to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nevGUF0GJDjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        # Assert is to confirm that the output is the concatenation of the hidden state from every time step, whereas hidden is simply the final hidden state\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0)) #forward RNN output of last token (output[-1,:,:])\n",
        "        \n",
        "        #Sqeeze is to remove the dimension of size 1  which would be passed to Fully connected layer for the prediction\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZp5Ubh-tsWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdTEwH7QuVXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89c0b093-2ade-4e0e-d0c8-cb564b8d3a88"
      },
      "source": [
        "# Check no. of trainable parameters\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmBNdZ-uWL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Optimizer is SGD with learing rate 1e-3...(what should be optimized, learning rate) \n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdxPVDU-3bse",
        "colab_type": "text"
      },
      "source": [
        "The Loss is defined using Criterion. Here we used logit with Binary Cross Entropy. Logit is to make sure the output predict is within the range 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNuvZP1Zufhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMHDz5ku0Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To calculate Accuracy\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ctq_7bNqARD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        ##Initialize the gradient as 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #prediction from the FC [batch_size,1] should be squeezed to remove 1 Dimension as creterion accepts only [batch_size]                \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        ##Loss function\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        ##Back propagation to change the gradient\n",
        "        loss.backward()\n",
        "        \n",
        "        ##Update the gradient\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqdRItBns7lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## In evaluation we will not update the gradient or change the gradient wts\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn5lJqa7tB6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soqAzglstEd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "0a35ce5b-63fe-4e2e-d884-b8ca2def8d10"
      },
      "source": [
        "## Run the model for the defined set of epochs\n",
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.694 | Train Acc: 50.37%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 48.78%\n",
            "Epoch: 02 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.86%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 49.21%\n",
            "Epoch: 03 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.91%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 49.26%\n",
            "Epoch: 04 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.94%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 48.94%\n",
            "Epoch: 05 | Epoch Time: 0m 45s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.24%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 50.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMzSl0OutIa5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bdb5a7a-912f-4ed6-afb9-3777c1aafcad"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.711 | Test Acc: 46.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmLXfB4uXQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}